{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9418/Week08/blob/main/COMP9418_W08_Gaussian_Factors_Solutions.ipynb)\n",
    "\n",
    "# Gaussian Factors\n",
    "\n",
    "**COMP9418 W08 Tutorial**\n",
    "\n",
    "- Instructor: Gustavo Batista\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Notebook designed by Gustavo Batista and Jeremy Gillen\n",
    "- Last Update 14th July 2024\n",
    "$\\newcommand{\\indep}{\\perp \\!\\!\\!\\perp}\\newcommand{\\b}{\\boldsymbol}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Prerequisites\n",
    "\n",
    "We will import the following modules for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial introduces Bayesian Networks that use continuous Gaussian instead of discrete random variables. \n",
    "\n",
    "We start by implementing a new factor class. Our previous library, `DiscreteFactors`, uses tables to represent a probability distribution, which is inappropriate for continuous variables. Therefore, we will need a new representation of probability distribution to incorporate continuous variables into our models.\n",
    "\n",
    "Unlike a table that can represent any discrete probability distribution, continuous distribution will require a specific representation. This tutorial follows the lecture content and implements a factor for the ubiquitous Gaussian distribution.\n",
    "\n",
    "This tutorial involves a large quantity of technical material summarised in Lecture 13. This lecture is based on the following content from the textbook *Probabilistic Graphical Models: Principles and Techniques*, by Koller & Friedman:\n",
    "\n",
    "1. Continuous variables, Section 5.5, pg. 185-190.\n",
    "2. \n",
    "Gaussian Network, Sectionss 7.1 & 7.2, pgs. 247-25.\n",
    "3. \r\n",
    "Variable Elimination in Gaussian NetworksSections , 14.1 & 14.2, pgs. 605-6.an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian distribution\n",
    "\n",
    "An $n$-dimensional Gaussian distribution is defined with it's mean vector $\\b\\mu$ and covariance matrix $\\Sigma$:\n",
    "$$ P(\\b x) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}}\\exp \\left(-\\frac{1}{2} (\\b x - \\b \\mu)^T\\Sigma^{-1}(\\b x - \\b \\mu)\\right). $$\n",
    "\n",
    "Note that $|\\Sigma|$ is the determinant of the covariance matrix.\n",
    "\n",
    "This figure illustrates a Gaussian distribution for two variables. The centre of this distribution is given by the vector $\\b \\mu$.\n",
    "\n",
    "![Bi-dimensional Gaussian](https://raw.githubusercontent.com/UNSW-COMP9418/Week09/main/img/Gaussian.png \"Gaussian distribution\")\n",
    "\n",
    "Another relevant parameter is the covariance matrix that defines the dependencies between the Gaussian dimensions. When the dimensions are independent, the Gaussian has a round shape. Stronger dependencies will force the Gaussian to have a more elliptical shape.\n",
    "\n",
    "![Contour plot of bi-dimensional Gaussians](https://raw.githubusercontent.com/UNSW-COMP9418/Week09/main/img/Covariance.png \"Gaussians with different covariance matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical representation for Gaussian distributions\n",
    "\n",
    "A multidimensional Gaussian distribution can also be represented with the following equation:\n",
    "\n",
    "$$ P(\\b x) = \\exp \\left(-\\frac{1}{2} \\b{x}^T K \\b x + \\b h^T \\b x + g\\right),$$\n",
    "\n",
    "where \n",
    "* $\\b x$ is an $n$-dimension vector representing the outcome of each variable\n",
    "* $K = \\Sigma^{-1}$ is an $n \\times n$ matrix\n",
    "* $\\b h = \\Sigma^{-1}\\b \\mu$ is an $n$-dimension vector\n",
    "* $g = -\\frac{1}{2}\\b \\mu^T\\Sigma^{-1}\\b \\mu - \\log\\left((2\\pi)^{n/2} |\\Sigma|^{1/2}\\right)$ is a scalar \n",
    "\n",
    "This representation is relevant because we can easily implement the joint (multiplication), marginalisation (sum) and setting evidence using this representation. We use the notation $\\mathcal{C}(K, \\b h, g)$ to denote this representation. To represent a Gaussian distribution in the canonical form, we just need to compute $K$, $\\b h$ and $g$ from the mean and covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's start creating a new class, `GaussianFactor`. The idea is that this class will have a lot of flexibility in how we initiate it. We can initiate the class in the following ways:\n",
    "\n",
    "1. As a Gaussian distribution with parameters $\\b \\mu$ and $\\Sigma$.\n",
    "2. As a Gaussian in canonical form with parameters $K$, $\\b h$ and $g$.\n",
    "3. As a conditional Gaussian distribution (we will discuss this part later).\n",
    "\n",
    "Let's create the first method, `__init__`, which will give us multiple ways to initialize the distribution, as described above. The idea is that no matter how the class is initialised, it will internally store a canonical representation of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFactor:\n",
    "    def __init__(self, domain, mu=None, sigma=None, parents=None, beta=None, b_mean=None, b_var=None, K=None, h=None, g=None):\n",
    "        '''\n",
    "        There are three ways to initialize this object.\n",
    "        1. As a multivariate Gaussian\n",
    "        domain:   A list or tuple of the names of each variable.\n",
    "        mu:       The mean vector\n",
    "        sigma:    The covariance matrix\n",
    "        \n",
    "        2. Directly with the canonical form of the parameters\n",
    "        domain:  A list or tuple of the names of each variable.\n",
    "        K:       see cell above (or Koller&Friedman textbook (section 14.2.1.1)) for definitions of these variables\n",
    "        h:\n",
    "        g:\n",
    "\n",
    "        3. As a conditional distribution Y|X where Y = beta^T X + b\n",
    "        domain:   List of names with the *child variable first*.\n",
    "        beta:     The vector of parameters that define the scale of each of the variables X_i\n",
    "        b_mean:   The mean of b\n",
    "        b_var:    The variance of b\n",
    "        '''\n",
    "        n = len(domain)\n",
    "        self.domain = domain\n",
    "        if mu is not None and sigma is not None:\n",
    "            mu, sigma = np.array(mu).reshape((n,)), np.array(sigma).reshape((n,n))\n",
    "            self.K = ... # TODO\n",
    "            self.h = ... # TODO\n",
    "            self.g = ... # TODO\n",
    "        elif K is not None and h is not None and g is not None:\n",
    "            self.K = np.array(K).reshape((n,n))\n",
    "            self.h = np.array(h).reshape((n,))\n",
    "            self.g = g\n",
    "        elif beta is not None and b_mean is not None and b_var is not None:\n",
    "            # We will complete the function below in the next exercise\n",
    "            K,h,g = self._init_as_conditional(domain[0],domain[1:], beta, b_mean, b_var)\n",
    "            self.K = K\n",
    "            self.h = h\n",
    "            self.g = g\n",
    "        else:\n",
    "            raise ValueError(\"Insufficient arguments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing class initialisation\n",
    "\n",
    "Let's now extend the `GaussianFactor` class with some useful methods:\n",
    "\n",
    "1. Calculate the density of a point.\n",
    "2. Plot the Gaussian (1 dimension) or contour diagram (2 dimensions).\n",
    "3. Return the mean.\n",
    "4. Return the covariance matrix.\n",
    "5. Copy the class object, returning a deep copy of the object.\n",
    "6. Returning a textual representation of the class used with the `print` function (useful for debugging purposes).\n",
    "\n",
    "All these methods are implemented below. There is no need for you to modify or add code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFactor(GaussianFactor):\n",
    "            \n",
    "    def density(self, x):\n",
    "        '''\n",
    "        x:  ndarray of shape [..., len(domain)], to specify the set of \n",
    "        points in the domain space for which the density should be returned.\n",
    "        \n",
    "        returns: ndarray of shape [...], same as input but missing final dimension.\n",
    "        '''\n",
    "        x = np.array(x)\n",
    "        if x.shape == tuple():\n",
    "            x = x.reshape(-1)\n",
    "        if len(self.domain) == 1 and x.shape[-1] != 1:\n",
    "            x = x.reshape((*x.shape,1))\n",
    "        assert(x.shape[-1] == len(self.domain))\n",
    "        output_shape = x.shape[:-1]\n",
    "        xT = np.array(x).reshape((*x.shape[:-1],1,len(self.domain)))\n",
    "        x = np.array(x).reshape((*x.shape[:-1],len(self.domain),1))\n",
    "        hT = self.h.reshape(1,len(self.domain))\n",
    "        return np.exp(-(1/2)*xT@self.K@x+hT@x+self.g).reshape(output_shape)\n",
    "    \n",
    "    def plot(self):\n",
    "        '''\n",
    "        If mean() and covariance() are well defined (and sometimes when they aren't),\n",
    "        this function will plot a graph or contour diagram of the distribution.\n",
    "        Limited to 1 or 2-dimensional factors.\n",
    "        '''\n",
    "        try:\n",
    "            mu = self.mean()\n",
    "        except np.linalg.LinAlgError:\n",
    "            raise np.linalg.LinAlgError(\"Can't plot conditional distributions\")\n",
    "            \n",
    "        if len(self.domain) == 1:\n",
    "            mu = self.mean()\n",
    "            sigma = np.sqrt(self.covariance())[0,0]\n",
    "            x = np.linspace(mu-5, mu+5, 100)\n",
    "            y = self.density(x)\n",
    "            plt.plot(x,y)\n",
    "            plt.xlabel(f'Domain: {self.domain[0]}')\n",
    "            plt.ylabel('density')\n",
    "            plt.show()\n",
    "        elif len(self.domain) == 2:\n",
    "            mu = self.mean()\n",
    "            x1 = np.linspace(mu[0]-5,mu[0]+5,200).reshape(-1,1)\n",
    "            x2 = np.linspace(mu[1]-5,mu[1]+5,200).reshape(1,-1)\n",
    "            a,b = np.meshgrid(x1,x2)\n",
    "            x = np.dstack([a,b])\n",
    "            y = self.density(x)\n",
    "            plt.contour(a,b, y)\n",
    "            plt.xlabel(f'Domain: {self.domain[0]}')\n",
    "            plt.ylabel(f'Domain: {self.domain[1]}')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Error: Need 1 or 2 dimensional gaussian for plotting\")\n",
    "            \n",
    "    def mean(self):\n",
    "        '''\n",
    "        Returns the mean of this Gaussian distribution, assuming it is well defined (it isn't if conditional).\n",
    "        '''\n",
    "        return np.linalg.inv(self.K)@self.h\n",
    "    \n",
    "    def covariance(self):\n",
    "        '''\n",
    "        Returns the covariance matrix of this Gaussian distribution, assuming it is well defined (it isn't if conditional).\n",
    "        '''\n",
    "        return np.linalg.inv(self.K)\n",
    "\n",
    "\n",
    "    def copy(self):\n",
    "        '''\n",
    "        Creates a full copy of the object\n",
    "        '''\n",
    "        return copy.deepcopy(self)\n",
    "            \n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Creates a string representation of the distribution.\n",
    "        Tries to print the mean and covariance.\n",
    "        If the distribution is conditional, the covariance matrix isn't well defined, so it resorts\n",
    "        to print the canonical form parameters.\n",
    "        '''\n",
    "        try:\n",
    "            return f\"Factor over {tuple(self.domain)},\\nμ = {self.mean()},\\nΣ = \\n{self.covariance()}\"\n",
    "        except np.linalg.LinAlgError:\n",
    "            return f\"Factor over {tuple(self.domain)},\\nK = \\n{self.K},\\nh = {self.h},\\ng = {self.g}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We can now test our code by creating a few Gaussians. The first is unidimensional with $\\b \\mu = 1$ and $\\Sigma = .8$. You should see a plot like this:\n",
    "\n",
    "![1-dimensional Gaussian](https://raw.githubusercontent.com/UNSW-COMP9418/Week09/main/img/1D-Gaussian.png \"1D Testing Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 dimensional factor\n",
    "a = GaussianFactor(('A',), mu=..., sigma=...)    # TODO\n",
    "a.plot()\n",
    "# Other testing information:\n",
    "print(f\"Density at A=0.5 is {a.density([.5])}\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is bidimensional with $\\b \\mu = [1, 2]$ and $\\Sigma = [[2, 1],[1, 2]]$. You should see a plot like this:\n",
    "\n",
    "![2-dimensional Gaussian](https://raw.githubusercontent.com/UNSW-COMP9418/Week09/main/img/2D-Gaussian.png \"2D Testing Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = ...            # TODO\n",
    "sigma = ...         # TODO\n",
    "b = GaussianFactor(('B', 'A'), mu, sigma)\n",
    "b.plot()\n",
    "# Other testing information:\n",
    "print(f\"Density at B=0,A=2 is {b.density([0,2])}\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Bayesian Networks\n",
    "\n",
    "The Gaussian Bayesian Network is a model for continuous variables. The figure below illustrates one of these models for a simple toy example with for variables: Inside temperature ($I$), Outside temperature ($O$), Sensor ($S$) and Energy ($E$). In this model, the sensor measures the internal temperature and the amount of energy spent to control the temperature is related to the inside and outside temperatures.\n",
    "\n",
    "![Gaussian Bayes Net](https://raw.githubusercontent.com/UNSW-COMP9418/Week09/main/img/GaussianBayesNet.png \"Example Gaussian Bayesian Network\")\n",
    "\n",
    "You probably have noticed that our current Gaussian factors are good for representing the unconditional probabilities for variables without parents, such as $I$ and $O$.\n",
    "\n",
    "However, we will need a different representation for a conditional Gaussian distribution. One representation often used is the linear Gaussian model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Gaussian Model\n",
    "\n",
    "A linear Gaussian model is a simple representation used for conditional Gaussian distributions. In this model:\n",
    "1. The mean is a linear combination of the parents' values.\n",
    "2. The variance is a parameter that is independent of the parents.\n",
    "\n",
    "Although simple, this model is often used in practice. For instance, the sensor ($S$) in the previous figure is modelled by $S \\sim  \\mathcal{N}(I, \\sigma^2_S)$. This means the sensor reads, on average, the same as the inside temperature $I$. This is equivalent to saying that the sensor is not biased (it does not systematically under or overestimate the inside temperature). The variance is the sensor error often reported by the manufacturer in its technical documentation.\n",
    "\n",
    "Similarly, the Energy ($E$) has a model $E \\sim \\mathcal{N}(\\beta_0 + \\beta_1 I + \\beta_2 O; \\sigma^2_E)$. This means that the energy is a quantity that linearly depends on the inside and outside temperatures, as we can expect the amount of energy to be related to the difference between these temperatures. Notice that the $\\beta$ parameters can be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's make a conditional factor using a linear Gaussian model. We want to define a variable $Y$ as a linear combination of parent Gaussian variables. This means $Y = \\beta_0 X_0 + \\beta_1 X_1 + ... \\beta_n X_n + b$, or equivalently $Y = \\beta^T X + b$. Each $X_i$ is a Gaussian variable of unknown mean and variance, and $b$ is a bias random variable with mean $\\mu_b$ and variance $\\sigma_b^2$.\n",
    "\n",
    "Then, we can represent this conditional distribution with the following values for $K$, $h$ and $g$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "K_{Y|X} &= \\begin{bmatrix}\n",
    "\\frac{1}{\\sigma_b^2} & -\\frac{1}{\\sigma^2_b}\\beta^T\\\\\n",
    "-\\frac{1}{\\sigma^2_b}\\beta & \\frac{1}{\\sigma^2_b}\\beta \\beta^T\\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "h_{Y|X} &= \\begin{bmatrix}\n",
    "\\frac{1}{\\sigma^2_b}\\mu_b\\\\\n",
    "-\\frac{1}{\\sigma^2_b}\\mu_b\\beta\\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "g_{Y|X} &= -\\frac{1}{2}\\left( \\frac{\\mu_b^2}{\\sigma^2_b} \\right)- \\frac{1}{2}\\log(2\\pi\\sigma_b^2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that $K_{Y|X}$ is a [Block Matrix](https://en.wikipedia.org/wiki/Block_matrix). The submatricies of $K_{Y|X}$ have shapes $\\begin{bmatrix}\n",
    "1\\times1 & 1\\times n\\\\\n",
    "n\\times 1 & n\\times n\\\\\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "Now it is your turn. We will implement the `_init_as_conditional` method, which is called by `__init__` when we want to create a conditional Gaussian factor. We have done most of the work and left some gaps to fill in using the equations shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFactor(GaussianFactor):\n",
    "    def _init_as_conditional(self, child, parents, beta, mean, var):\n",
    "        '''\n",
    "        This function initialises the factor as a conditional distribution P(Y|X),\n",
    "        where Y = \\beta^T X + b. The __init__ function will be the only function to call it.\n",
    "        Arguments:\n",
    "            child:     Name of Y\n",
    "            parents:   Names of each X\n",
    "            beta:      vector to multiply with X\n",
    "            mean:      mean of b\n",
    "            var:       variance of b\n",
    "        '''\n",
    "        n = len(beta)\n",
    "        beta = np.array(beta).reshape((n,1)) # make sure beta is a column vector\n",
    "        K = np.zeros((n+1,n+1))\n",
    "        \n",
    "        # top left section of K\n",
    "        K[0,0] = ... # TODO\n",
    "        # top right section of K\n",
    "        K[0:1,1:] = ... # TODO\n",
    "        # bottom left section of K\n",
    "        K[1:,0:1] = ... # TODO\n",
    "        # bottom right section of K\n",
    "        K[1:,1:] = ... # TODO\n",
    "        \n",
    "        h = np.zeros((n+1,1))\n",
    "        # top section of h\n",
    "        h[0,:] = ... # TODO\n",
    "        # bottom section of h\n",
    "        h[1:,:] = ... # TODO\n",
    "        \n",
    "        # reshape h to be row vector (for consistency with previous format)\n",
    "        h = h.reshape(-1)\n",
    "        \n",
    "        g = -(1/2)*(mean**2/var) - (1/2)*np.log(2*np.pi*var\n",
    "        \n",
    "        return K,h,g\n",
    "\n",
    "################\n",
    "# Test code\n",
    "################\n",
    "\n",
    "b = GaussianFactor(('B', 'A'), beta=[2], b_mean=1, b_var=0.3**2)\n",
    "print(\"K = \\n\", b.K)\n",
    "print(\"h =\", b.h)\n",
    "print(\"g =\", b.g)\n",
    "print()\n",
    "\n",
    "e = GaussianFactor(('B', 'A', 'D'), beta=[5,2], b_mean=-1, b_var=0.5**2)\n",
    "print(\"K = \\n\", e.K)\n",
    "print(\"h =\", e.h)\n",
    "print(\"g =\", e.g)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your solution is correct, you should see the following output:\n",
    "```\n",
    "K = \n",
    " [[ 11.11111111 -22.22222222]\n",
    " [-22.22222222  44.44444444]]\n",
    "h = [ 11.11111111 -22.22222222]\n",
    "g = -5.270521284434292\n",
    "\n",
    "K = \n",
    " [[  4. -20.  -8.]\n",
    " [-20. 100.  40.]\n",
    " [ -8.  40.  16.]]\n",
    "h = [-4. 20.  8.]\n",
    "g = -2.2257913526447273\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Gaussian Factors\n",
    "\n",
    "Our new class can now represent conditional and unconditional Gaussian distributions using the canonical form. It is time to start implementing the three main operations: join, marginalisation and setting evidence.\n",
    "\n",
    "We start with join. Before coding this operation, we will need a helper function. In tutorial 2, before we multiplied two arrays, we had to reorder the domain of a factor and extend it to include new variables. We will make a separate function for this purpose since we need to be able to reorder the domain for the join, marginalize, and evidence functions.\n",
    "\n",
    "We need a way to do this for Gaussian factors. Fortunately, it's not too complicated. We will implement a method for you called `_extend`. This method receives as an argument the new domain that it needs to have and shuffles up $K$, $h$, and $g$ to match the order of the new domain. If the domain contains new variables not in the current domain, we can simply put zeros in the corresponding locations of $K$ and $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFactor(GaussianFactor):\n",
    "    def _extend(self, new_domain):\n",
    "        '''\n",
    "        This function is for adding variables to the domain or reordering the variables in the domain\n",
    "        Note that self.domain must contain a subset of the variables in new_domain\n",
    "        '''\n",
    "        n = len(new_domain)\n",
    "        \n",
    "        # add zeros to K and h corresponding to the new variables\n",
    "        new_K = np.zeros((len(new_domain),len(new_domain)))\n",
    "        new_K[:len(self.domain), :len(self.domain)] = self.K\n",
    "        new_h = np.zeros(len(new_domain))\n",
    "        new_h[:len(self.domain)] = self.h\n",
    "        new_g = self.g\n",
    "        old_order = list(self.domain) + list(set(new_domain)-set(self.domain))\n",
    "        # shuffle rows and columns of K according to new order\n",
    "        new_order = []\n",
    "        for v in new_domain:\n",
    "            new_order.append(old_order.index(v))\n",
    "        new_K = new_K[new_order,:]\n",
    "        new_K = new_K[:,new_order]\n",
    "\n",
    "        # shuffle h according to new order\n",
    "        new_h = new_h[new_order]\n",
    "        \n",
    "        return self.__class__(new_domain, K=new_K, h=new_h, g=new_g)\n",
    "        \n",
    "####################\n",
    "# Demonstration code\n",
    "####################\n",
    "\n",
    "e = GaussianFactor(('B', 'A', 'D'), beta=[5,2], b_mean=-1, b_var=0.5**2)\n",
    "print(\"K = \\n\", e.K)\n",
    "print(\"h =\", e.h)\n",
    "print(\"g =\", e.g)\n",
    "print()\n",
    "\n",
    "f = e._extend(('D', 'A', 'B', 'E'))\n",
    "print(\"K = \\n\", f.K)\n",
    "print(\"h =\", f.h)\n",
    "print(\"g =\", f.g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Finally, we are ready to implement `join`. The math is really simple. If we have two factors in the canonical form, we can simply add the internal parameters:\n",
    "\n",
    "$$\\mathcal{C}(K_1,\\b h_1,g_1) \\times \\mathcal{C}(K_2,\\b h_2, g_2) = \\mathcal{C}(K_1 + K_2, \\b h_1 + \\b h_2, g_1 + g_2)$$\r\n",
    "Notice that before summing $K$ and $\\b h$, we need to ensure they have the same dimensions and variables order. We use the recently coded `_extend` method to ensure that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFactor(GaussianFactor):\n",
    "    def join(self, other):\n",
    "        '''\n",
    "        This function multiplies two factors.\n",
    "        '''\n",
    "        new_domain = list(self.domain) + list(set(other.domain)-set(self.domain))\n",
    "        \n",
    "        # extend the domain of self and other\n",
    "        new_self = self._extend(new_domain)\n",
    "        new_other = other._extend(new_domain)\n",
    "        \n",
    "        # Calculate the new values of K, h and g\n",
    "        K = ... # TODO\n",
    "        h = ... # TODO\n",
    "        g = ... # TODO\n",
    "        \n",
    "        # return new joined factor\n",
    "        return self.__class__(new_domain,K=K,h=h,g=g)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        '''\n",
    "        Override the * operator, so that it can be used to join factors\n",
    "        '''\n",
    "        return self.join(other)\n",
    "\n",
    "################\n",
    "# Test code\n",
    "################\n",
    "\n",
    "a = GaussianFactor(('A',), mu=5., sigma=0.8**2)\n",
    "b = GaussianFactor(('B','A'), beta=[2], b_mean=3, b_var=0.3**2)\n",
    "c = a.join(b)\n",
    "print(a)\n",
    "print(c)\n",
    "c.plot()\n",
    "\n",
    "d = GaussianFactor(('A',), mu=0, sigma=0.5**2)\n",
    "e = d*a\n",
    "print(e)\n",
    "e.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "```\n",
    "Factor over ('A', 'B'),\n",
    "μ = [ 5. 13.],\n",
    "Σ = \n",
    "[[0.64 1.28]\n",
    " [1.28 2.65]]\n",
    "\n",
    "Factor over ('A',),\n",
    "μ = [1.40449438],\n",
    "Σ = \n",
    "[[0.17977528]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginalising Factors\n",
    "\n",
    "Now, we will implement the marginalize function. This involves splitting up the domain into:\n",
    "\n",
    "1. A set of variables to keep, which we will call $x$, and\n",
    "2. A variable to integrate, which we will call $y$.\n",
    "\n",
    "The equations to update $K$, $h$ and $g$ are:\n",
    "$$\n",
    "\\begin{align}\n",
    "K' &= K_{xx} - K_{xy}K_{yy}^{-1}K_{yx} \\\\\n",
    "h' &= h_{x} - K_{xy}K_{yy}^{-1}h_y \\\\\n",
    "g' &= g + \\frac{1}{2}\\left(\\log |2\\pi K_{yy}^{-1}| + h_y^T K_{yy}^{-1}h_y\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now it is your turn. Complete the code for the method `marginalize` below. We have done most of the work for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GaussianFactor(GaussianFactor):\n",
    "    def marginalize(self, var):\n",
    "        '''\n",
    "        This function integrates out one variable from the domain\n",
    "        '''\n",
    "        n = len(self.domain)\n",
    "        new_domain = list(self.domain)\n",
    "        new_domain.remove(var)\n",
    "        f = self._extend(new_domain+[var]) #reorder the variables to put var last\n",
    "        \n",
    "        # Select submatricies from K and h\n",
    "        K_yy_inv = np.linalg.inv(f.K[-1:,-1:])\n",
    "        K_xx = f.K[:-1,:-1]\n",
    "        K_xy = f.K[:-1,-1:]\n",
    "        K_yx = f.K[-1:,:-1]\n",
    "        h_x = f.h[:-1]\n",
    "        h_y = f.h[-1:]\n",
    "        \n",
    "        # Use the submatricies above to calculate the updated K, h and g\n",
    "        new_K = ... # TODO\n",
    "        new_h = ... # TODO\n",
    "        new_g = ... # TODO\n",
    "        \n",
    "        return self.__class__(new_domain, K=new_K, h=new_h,g=new_g)\n",
    "\n",
    "################\n",
    "# Test code\n",
    "################\n",
    "\n",
    "a = GaussianFactor(('A', 'B'), mu=[5., 13.], sigma=[[0.64, 1.28],[1.28, 2.65]])\n",
    "print(a)\n",
    "b = a.marginalize('A')\n",
    "print(b)\n",
    "\n",
    "d = GaussianFactor(('D',), mu=1, sigma=0.2)\n",
    "e = GaussianFactor(('B', 'A', 'D'), beta=[5,2], b_mean=-1, b_var=0.5**2)\n",
    "f = e*a*d\n",
    "print(f)\n",
    "f = f.marginalize('A').marginalize('D')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "```\n",
    "Factor over ('A', 'B'),\n",
    "μ = [ 5. 13.],\n",
    "Σ = \n",
    "[[0.64 1.28]\n",
    " [1.28 2.65]]\n",
    "\n",
    "Factor over ('B',),\n",
    "μ = [13.],\n",
    "Σ = \n",
    "[[2.65]]\n",
    "\n",
    "Factor over ('B', 'A', 'D'),\n",
    "μ = [5.93478261 1.3826087  0.24637681],\n",
    "Σ = \n",
    "[[ 0.61195652  0.23652174 -0.2173913 ]\n",
    " [ 0.23652174  0.10573913 -0.11130435]\n",
    " [-0.2173913  -0.11130435  0.17681159]]\n",
    "\n",
    "Factor over ('B',),\n",
    "μ = [5.93478261],\n",
    "Σ = \n",
    "[[0.61195652]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting evidence\n",
    "\n",
    "This is our last method implementation for the `GaussanFactor` class: the evidence method.\n",
    "\n",
    "Similar to `marginalize`, we need to split the domain into parts. The set $x$ will represent the variables we are keeping, and $y$ will represent the variables we are setting to a specific value. The specific values are specified by a vector $\\b y$.\n",
    "Then the equations to update $K$, $h$ and $g$ are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "K' &= K_{xx} \\\\\n",
    "\\b h' &= \\b h_{x} - K_{xy} \\b y \\\\\n",
    "g' &= g + h_y^T \\b y - \\frac{1}{2} \\b y^T K_{yy} \\b y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now it is your turn. Do not worry; we have done most of the heavy lifting for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFactor(GaussianFactor):\n",
    "    def evidence(self, **kwargs):\n",
    "        '''\n",
    "        Sets evidence which results in removal of the evidence variables\n",
    "        This function must be used to set evidence on all factors before any are joined,\n",
    "        because it removes the evidence variables from the factor\n",
    "        \n",
    "        Usage: fac.evidence(A=4.6,B=-0.3)\n",
    "        This returns a factor which has set the variable 'A' to '.6 and 'B' to -0.3.\n",
    "        '''\n",
    "        evidence_dict = kwargs\n",
    "        \n",
    "        if(len(evidence_dict) == 0):\n",
    "            return self\n",
    "        \n",
    "        # work out new domain vars and evidence vars\n",
    "        new_domain = list(self.domain)\n",
    "        evidence_vars = []\n",
    "        evidence_values = []\n",
    "        for var,value in evidence_dict.items():\n",
    "            new_domain.remove(var)\n",
    "            evidence_vars.append(var)\n",
    "            evidence_values.append(value)\n",
    "            \n",
    "        # rearrange the domain to put evidence vars last\n",
    "        f = self._extend(new_domain+evidence_vars)\n",
    "        \n",
    "        # Split up K and h into sections\n",
    "        n = len(evidence_vars)\n",
    "        m = len(new_domain)\n",
    "        K_xx = f.K[:m,:m]\n",
    "        K_xy = f.K[:m,-n:]\n",
    "        K_yy = f.K[-n:,-n:]\n",
    "        h_x = f.h[:m]\n",
    "        h_y = f.h[-n:]\n",
    "        # put evidence variables into a vector called y\n",
    "        y = np.array(evidence_values)\n",
    "        \n",
    "        # update variables\n",
    "        new_K = ... # TODO\n",
    "        new_h = ... # TODO\n",
    "        new_g = ... # TODO\n",
    "        return self.__class__(new_domain, K=new_K, h=new_h,g=new_g)\n",
    "    \n",
    "################\n",
    "# Test code\n",
    "################\n",
    "\n",
    "a = GaussianFactor(('A',), mu=5., sigma=0.8**2)\n",
    "print(a)\n",
    "print()\n",
    "b = GaussianFactor(('B', 'A'), beta=[2], b_mean=3, b_var=0.3**2)\n",
    "c = a.join(b)\n",
    "print(c)\n",
    "c.plot()\n",
    "print(c.evidence(A=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution should match:\n",
    "```\n",
    "Factor over ('A',),\n",
    "μ = [5.],\n",
    "Σ = \n",
    "[[0.64]]\n",
    "\n",
    "Factor over ('A', 'B'),\n",
    "μ = [ 5. 13.],\n",
    "Σ = \n",
    "[[0.64 1.28]\n",
    " [1.28 2.65]]\n",
    "\n",
    "Factor over ('B',),\n",
    "μ = [9.],\n",
    "Σ = \n",
    "[[0.09]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional methods\n",
    "\n",
    "Congratulations! We are done with the `GaussianFactor` class. We add two methods for completeness. `sample` generates one sample from the distribution specified in the Gaussian factor. `normalize` normalizes a factor and it is particularly useful after we set evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GaussianFactor(GaussianFactor):\n",
    "    def sample(self, **kwargs):\n",
    "        '''\n",
    "        Draw a sample from this distribution, given evidence.\n",
    "        output: dict containing a map from variable names to values\n",
    "        '''\n",
    "        f = self.evidence(**kwargs)\n",
    "        sample = np.random.multivariate_normal(f.mean(), f.covariance())\n",
    "        sample_dict = dict((var, sample[i]) for i,var in enumerate(f.domain))\n",
    "        return sample_dict\n",
    "    \n",
    "    def normalize(self):\n",
    "        '''\n",
    "        Normalize the factor (will only work if the covariance is well-defined)\n",
    "        '''\n",
    "        n = len(self.domain)\n",
    "        self.g = -0.5*self.mean().T@self.h - np.log((2*np.pi)**(n/2)*np.linalg.det(self.covariance())**(1/2))\n",
    "        return self\n",
    "\n",
    "\n",
    "#################\n",
    "# Demonstration\n",
    "#################\n",
    "\n",
    "# Compare multivariate sampling and plotting, after \n",
    "a = GaussianFactor(('A',),mu=1,sigma=0.8)\n",
    "b = GaussianFactor(('B','A'),beta=[2],b_mean=1,b_var=0.3**2)\n",
    "c = a.join(b)\n",
    "print(c)\n",
    "c.plot()\n",
    "samples = [c.sample() for i in range(100)]\n",
    "x = [s['A'] for s in samples]\n",
    "y = [s['B'] for s in samples]\n",
    "plt.plot(x,y,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 from the theory part\n",
    "\n",
    "Let's solve Question 1 from the theory part and use it to develop a simple approach to replying to probabilistic queries using the sampling method from the previous cell. The next cell restates the problem description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You received a job offer and decided to move to Alaska. The only issue is that you have a beloved tropical freshwater fish tank that requires a temperature between 24$^{\\circ}$C and 27$^{\\circ}$C. Before travelling, you decided to create a temperature control system. You went to a local electronics store to order a digital temperature sensor and decided to take three different models: $S_A$, $S_B$ and $S_C$.\n",
    "\n",
    "$S_A$ and $S_B$ are cheaper sensors that seem reasonably accurate for the temperature range you are interested in. You read the technical specification and found these plots:\n",
    "\n",
    "![Temperature sensors](https://raw.githubusercontent.com/UNSW-COMP9418/Week09/main/img/Temperature_sensors.png \"Temperature sensors\")\n",
    "\n",
    "$S_C$ is a more expensive sensor, and the manufacturer claims that it is an unbiased sensor, i.e., the sensor measures the actual temperature on average. Also, the manufacturers claim that the sensors reading variance for $S_A$ is $.2^{2\\circ}$C, $S_B$ is $.3^{2\\circ}$C and $S_C$ is $.5^{2\\circ}$C.\n",
    "\n",
    "You would like to know the probability that the actual temperature is less than 24$^{\\circ}$C when the three sensors read this same temperature.\n",
    "\n",
    "Design a Bayesian Gaussian network for this problem. You reckon that the average indoor temperature at your Alaska home will be $25^\\circ$C, but as you never lived there, you want to assign a large variance of $5^{2\\circ}$C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's implement This Gaussian Bayesian network in the next cell and plot the resulting distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = GaussianFactor(...)\n",
    "SA = GaussianFactor(...)\n",
    "SB = GaussianFactor(...)\n",
    "SC = GaussianFactor(..)\n",
    "\n",
    "# Join factors and set evidence\n",
    "\n",
    "F = ...\n",
    "\n",
    "F.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the temperature will likely be below the minimum desired temperature. Can you explain why this is the case?\n",
    "\n",
    "To get a numerical response to this question, we can use the ``sample`` method implemented before. The idea is to use a Monte Carlo approach that generates a large set of data points, computing how many are below $24^\\circ$C.\n",
    "\n",
    "The next cell assumes that the factor ``F`` stores the resulting Gaussian distribution after joining and setting evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [F.sample() for i in range(10000)]\n",
    "temperatures = np.array([s['T'] for s in samples])\n",
    "counts = temperatures < 24\n",
    "print(f'The probability of having temperature below 24C is {counts.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo approach\n",
    "\n",
    "Monte Carlo approaches will be further explored in the next lecture. For now, we notice that these approach are highly flexible and can help us to easily compute probabilities in higher dimensional distributions. \n",
    "\n",
    "We conclude with a small challenge. For the previous exercise, how can we use Monte Carlo simulation to answer the query: what is the probability that the aquarium temperature is above 24C given that all three sensors read temperatures above this same temperature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's use the Monte Carlo simulation to answer the query above. It will allow us to not set evidence for a particular sensor reading but to have a more complex situation where the sensors read above a particular temperature.\n",
    "\n",
    "You only need to obtain the joint probability distribution and use sampling to compute the probability given the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = GaussianFactor(...)\n",
    "SA = GaussianFactor(...)\n",
    "SB = GaussianFactor(...)\n",
    "SC = GaussianFactor(..)\n",
    "\n",
    "# Join factors but do not set evidence\n",
    "\n",
    "F = ...\n",
    "\n",
    "# Select the samples that the sensors have all reading above 24C\n",
    "\n",
    "# Compute the fraction of these readings in each the temperature is below 24C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "That is all for today! In the next tutorial, we will explore more Monte Carlo simulations and see how we can approximate probabilistic queries using forward, rejection, likelihood, and Gibbs sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
